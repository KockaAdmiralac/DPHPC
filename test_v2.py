from argparse import ArgumentParser
import json
import random
from typing import List

import marshmallow_dataclass
import preparation
import runner
import single_benchmark
from structures import *


class BenchmarkRunner:
    r: runner.Runner

    benchmark_config: BenchmarkConfiguration
    prep: PreparationResult
    notified_finished_must_completes: bool = False

    def __init__(self, benchmark_config: BenchmarkConfiguration) -> None:
        self.benchmark_config = benchmark_config

    def select_next_benchmark(self) -> SingleBenchmark | None:
        if len(self.prep.must_completes):
            idx = random.randint(0, len(self.prep.must_completes) - 1)
            item = self.prep.must_completes[idx]
            del self.prep.must_completes[idx]
            return item

        if not self.notified_finished_must_completes:
            print("Completed minimum required runs")
            self.notified_finished_must_completes = True
        if self.prep.keep_going:
            return random.choice(self.prep.benchmark_choices)
        else:
            return None

    def run_single_benchmark(self, b: SingleBenchmark) -> ProcessedResult:
        try:
            return single_benchmark.run_benchmark(b, self.prep)
        except Exception as e:
            print(f"Couldn't run {b.compile_settings.binary_path}")
            raise e

    def main_run_loop(self, r: runner.Runner) -> List[ProcessedResult]:
        results: List[ProcessedResult] = []
        while True:
            try:
                next_benchmark = self.select_next_benchmark()
                if next_benchmark is None:
                    break
                res = self.run_single_benchmark(next_benchmark)

                results.append(res)
            except KeyboardInterrupt:
                break

        return results

    def main_run(self, args):
        r = runner.Runner()
        self.prep = preparation.all_prepare(r, self.benchmark_config)
        results = self.main_run_loop(r)
        procres_schema = marshmallow_dataclass.class_schema(ProcessedResult)()
        results_pyobj = list(map(procres_schema.dump, results))
        with open(args.results_output, "w+") as f:
            json.dump(results_pyobj, f, indent=4)


if __name__ == "__main__":
    # So far you start the program by copying the output of gen_benchmark_config.py in place of BenchmarkConfiguration here.
    # In future you have the option of loading this configuration from a JSON file.

    parser = ArgumentParser(
        description="Runs a set of benchmarks following a provided configuration."
    )

    parser.add_argument(
        "--config",
        required=True,
        help="A configuraion file in JSON format, normally generated by gen_benchmark_config.py",
    )

    parser.add_argument(
        "--results-output", help="Where to save the JSON list of ProcessedResults"
    )

    args = parser.parse_args()

    with open(args.config, "r") as f:
        raw_benchmark_config = json.load(f)

    benchmark_config_schema = marshmallow_dataclass.class_schema(
        BenchmarkConfiguration
    )()
    bc = benchmark_config_schema.load(raw_benchmark_config)

    br = BenchmarkRunner(bc)
    br.main_run(args)
